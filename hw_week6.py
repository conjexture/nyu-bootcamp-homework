{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7e50351-15c7-4379-9369-cd41cd7ac272",
   "metadata": {},
   "source": [
    "# (Homework) Week 6 - DataScience Bootcamp Fall 2025\n",
    "\n",
    "**Name:** [Your Name]\n",
    "**Email:** [Your Email]\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911ae2a1-9b4d-4b8e-87a8-fd32d8c107c8",
   "metadata": {},
   "source": [
    "### Problem 1: Dataset Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65cfdb6-aca2-4dd7-aaa4-70fa30af475e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 1 Solution\n",
    "\n",
    "print(\"Problem 1: Dataset Splitting\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n1. Original Dataset Split Strategy:\")\n",
    "print(\"-\" * 60)\n",
    "print(\"Dataset: 44 phones × 100 people × ~200 recordings/day × 5 days\")\n",
    "print(\"Total recordings per person: ~1000\")\n",
    "print(\"Total recordings: ~100,000\")\n",
    "print()\n",
    "print(\"Strategy: SPEAKER-BASED SPLIT (ensures generalization to new speakers)\")\n",
    "print()\n",
    "print(\"Split by PEOPLE (not by recordings):\")\n",
    "print(\"  - Training set: 70 people (70,000 recordings)\")\n",
    "print(\"  - Validation set: 15 people (15,000 recordings)\")\n",
    "print(\"  - Test set: 15 people (15,000 recordings)\")\n",
    "print()\n",
    "print(\"Reasoning:\")\n",
    "print(\"  • Ensures NO speaker overlap between train/val/test\")\n",
    "print(\"  • Tests model's ability to generalize to UNSEEN speakers\")\n",
    "print(\"  • Prevents data leakage from same speaker across splits\")\n",
    "print(\"  • Validation set used for hyperparameter tuning\")\n",
    "print(\"  • Test set only used for final evaluation\")\n",
    "\n",
    "print(\"\\n2. Adding Kilian's Dataset:\")\n",
    "print(\"-\" * 60)\n",
    "print(\"New data: 10,000 recordings from single speaker (Kilian)\")\n",
    "print()\n",
    "print(\"Strategy: MIXED TRAINING + SPEAKER-SPECIFIC FINE-TUNING\")\n",
    "print()\n",
    "print(\"Step 1 - Split Kilian's data:\")\n",
    "print(\"  - Kilian training: 7,000 recordings\")\n",
    "print(\"  - Kilian validation: 1,500 recordings\")\n",
    "print(\"  - Kilian test: 1,500 recordings\")\n",
    "print()\n",
    "print(\"Step 2 - Training approach:\")\n",
    "print(\"  • Train base model on original 70 people (general model)\")\n",
    "print(\"  • Fine-tune on Kilian's 7,000 recordings (speaker adaptation)\")\n",
    "print(\"  • Validate on Kilian's 1,500 validation set\")\n",
    "print()\n",
    "print(\"Step 3 - Evaluation:\")\n",
    "print(\"  • Test on original 15-person test set (general performance)\")\n",
    "print(\"  • Test on Kilian's 1,500 test set (Kilian-specific performance)\")\n",
    "print()\n",
    "print(\"Reasoning:\")\n",
    "print(\"  • Base model provides general speaker-independent features\")\n",
    "print(\"  • Fine-tuning adapts to Kilian's specific voice characteristics\")\n",
    "print(\"  • Maintains generalization through pre-training\")\n",
    "print(\"  • Two test sets measure both objectives\")\n",
    "print(\"  • Prevents overfitting to Kilian through proper validation\")\n",
    "\n",
    "print(\"\\nAlternative: Multi-task Learning\")\n",
    "print(\"  • Train jointly on all data with speaker ID as auxiliary task\")\n",
    "print(\"  • Use loss weighting to balance general vs Kilian-specific performance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217b7930-1fef-4fd2-ac71-1467e8b165e8",
   "metadata": {},
   "source": [
    "### Problem 2: K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80f66d2-4e36-4e30-8ef5-72d9b7986ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "print(\"Problem 2: K-Nearest Neighbors\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Part 1: 1-NN Classification\n",
    "print(\"\\n1. 1-NN Classification and Decision Boundary\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "positive = np.array([[1, 2], [1, 4], [5, 4]])\n",
    "negative = np.array([[3, 1], [3, 2]])\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(positive[:, 0], positive[:, 1], c='blue', s=200, marker='o', \n",
    "            edgecolors='black', linewidths=2, label='Positive', zorder=3)\n",
    "plt.scatter(negative[:, 0], negative[:, 1], c='red', s=200, marker='s', \n",
    "            edgecolors='black', linewidths=2, label='Negative', zorder=3)\n",
    "\n",
    "# Create decision boundary\n",
    "x_min, x_max = 0, 6\n",
    "y_min, y_max = 0, 5\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n",
    "                     np.linspace(y_min, y_max, 200))\n",
    "\n",
    "all_points = np.vstack([positive, negative])\n",
    "all_labels = np.array([1, 1, 1, 0, 0])\n",
    "\n",
    "Z = np.zeros(xx.shape)\n",
    "for i in range(xx.shape[0]):\n",
    "    for j in range(xx.shape[1]):\n",
    "        test_point = np.array([xx[i, j], yy[i, j]])\n",
    "        distances = np.sqrt(np.sum((all_points - test_point)**2, axis=1))\n",
    "        Z[i, j] = all_labels[np.argmin(distances)]\n",
    "\n",
    "plt.contourf(xx, yy, Z, alpha=0.3, cmap=ListedColormap(['#FFAAAA', '#AAAAFF']))\n",
    "plt.contour(xx, yy, Z, colors='black', linewidths=1, alpha=0.5)\n",
    "\n",
    "plt.xlabel('Feature 1', fontsize=12)\n",
    "plt.ylabel('Feature 2', fontsize=12)\n",
    "plt.title('1-NN Decision Boundary', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Decision boundary plotted above.\")\n",
    "print(\"Blue regions: classified as Positive\")\n",
    "print(\"Red regions: classified as Negative\")\n",
    "\n",
    "# Part 2: Feature Scaling\n",
    "print(\"\\n2. Feature Scaling Impact\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "positive_scaled = np.array([[100, 2], [100, 4], [500, 4]])\n",
    "negative_scaled = np.array([[300, 1], [300, 2]])\n",
    "test_point = np.array([500, 1])\n",
    "\n",
    "print(\"Dataset:\")\n",
    "print(f\"Positive: {positive_scaled.tolist()}\")\n",
    "print(f\"Negative: {negative_scaled.tolist()}\")\n",
    "print(f\"Test point: {test_point.tolist()}\")\n",
    "\n",
    "# Before scaling\n",
    "all_scaled = np.vstack([positive_scaled, negative_scaled])\n",
    "labels_scaled = np.array([1, 1, 1, 0, 0])\n",
    "distances_before = np.sqrt(np.sum((all_scaled - test_point)**2, axis=1))\n",
    "nearest_before = np.argmin(distances_before)\n",
    "\n",
    "print(\"\\nBEFORE SCALING:\")\n",
    "for i, (point, label, dist) in enumerate(zip(all_scaled, labels_scaled, distances_before)):\n",
    "    class_name = \"Positive\" if label == 1 else \"Negative\"\n",
    "    print(f\"  Distance to {point}: {dist:.2f} ({class_name})\")\n",
    "print(f\"\\n  Nearest neighbor: {all_scaled[nearest_before]} ({['Negative', 'Positive'][labels_scaled[nearest_before]]})\")\n",
    "print(f\"  Classification: {'POSITIVE' if labels_scaled[nearest_before] == 1 else 'NEGATIVE'}\")\n",
    "\n",
    "# After scaling\n",
    "min_vals = all_scaled.min(axis=0)\n",
    "max_vals = all_scaled.max(axis=0)\n",
    "all_scaled_norm = (all_scaled - min_vals) / (max_vals - min_vals)\n",
    "test_point_norm = (test_point - min_vals) / (max_vals - min_vals)\n",
    "\n",
    "distances_after = np.sqrt(np.sum((all_scaled_norm - test_point_norm)**2, axis=1))\n",
    "nearest_after = np.argmin(distances_after)\n",
    "\n",
    "print(\"\\nAFTER SCALING [0,1]:\")\n",
    "for i, (point, label, dist) in enumerate(zip(all_scaled_norm, labels_scaled, distances_after)):\n",
    "    class_name = \"Positive\" if label == 1 else \"Negative\"\n",
    "    print(f\"  Distance to {point}: {dist:.4f} ({class_name})\")\n",
    "print(f\"\\n  Nearest neighbor: {all_scaled_norm[nearest_after]} ({['Negative', 'Positive'][labels_scaled[nearest_after]]})\")\n",
    "print(f\"  Classification: {'POSITIVE' if labels_scaled[nearest_after] == 1 else 'NEGATIVE'}\")\n",
    "\n",
    "print(\"\\nConclusion: Scaling changes the classification because it equalizes\")\n",
    "print(\"the influence of both features. Without scaling, Feature 1 (range 400)\")\n",
    "print(\"dominates Feature 2 (range 3) in distance calculations.\")\n",
    "\n",
    "# Part 3: Handling Missing Values\n",
    "print(\"\\n3. Handling Missing Features\")\n",
    "print(\"-\" * 60)\n",
    "print(\"Strategies for K-NN with missing values:\")\n",
    "print()\n",
    "print(\"a) IGNORE MISSING DIMENSIONS:\")\n",
    "print(\"   - Compute distance only on available features\")\n",
    "print(\"   - For point x with missing feature i: d(x,y) = sqrt(sum over j≠i of (x_j - y_j)²)\")\n",
    "print(\"   - Normalize by number of compared features\")\n",
    "print()\n",
    "print(\"b) IMPUTATION:\")\n",
    "print(\"   - Replace missing values with mean/median of k nearest neighbors\")\n",
    "print(\"   - Use neighbors found from non-missing features\")\n",
    "print(\"   - Then apply standard K-NN\")\n",
    "print()\n",
    "print(\"c) WEIGHTED DISTANCE:\")\n",
    "print(\"   - Weight each dimension by availability\")\n",
    "print(\"   - Give lower weight to dimensions missing in test point\")\n",
    "print()\n",
    "print(\"d) MULTIPLE MODELS:\")\n",
    "print(\"   - Train separate K-NN for each missing pattern\")\n",
    "print(\"   - Use model matching test point's missing pattern\")\n",
    "print()\n",
    "print(\"Best approach: (a) or (b), depending on missing data percentage\")\n",
    "\n",
    "# Part 4: High-dimensional Data\n",
    "print(\"\\n4. K-NN in High Dimensions (Images)\")\n",
    "print(\"-\" * 60)\n",
    "print(\"Why K-NN works well for images despite thousands of pixels:\")\n",
    "print()\n",
    "print(\"1. MANIFOLD HYPOTHESIS:\")\n",
    "print(\"   - Natural images lie on low-dimensional manifolds\")\n",
    "print(\"   - Effective dimensionality << actual dimensionality\")\n",
    "print(\"   - Similar images cluster together in pixel space\")\n",
    "print()\n",
    "print(\"2. CORRELATION STRUCTURE:\")\n",
    "print(\"   - Pixels are highly correlated (not independent)\")\n",
    "print(\"   - Adjacent pixels have similar values\")\n",
    "print(\"   - Reduces effective degrees of freedom\")\n",
    "print()\n",
    "print(\"3. SIGNAL-TO-NOISE:\")\n",
    "print(\"   - Important visual features (edges, textures) create strong signals\")\n",
    "print(\"   - These signals dominate distance calculations\")\n",
    "print(\"   - Irrelevant dimensions contribute mostly noise\")\n",
    "print()\n",
    "print(\"4. LARGE SAMPLE SIZES:\")\n",
    "print(\"   - Image datasets often have many examples\")\n",
    "print(\"   - Helps mitigate curse of dimensionality\")\n",
    "print(\"   - Increases chance of finding true neighbors\")\n",
    "print()\n",
    "print(\"5. SEMANTIC SIMILARITY:\")\n",
    "print(\"   - Euclidean distance in pixel space correlates with semantic similarity\")\n",
    "print(\"   - Small pixel changes = visually similar images\")\n",
    "print()\n",
    "print(\"Limitations:\")\n",
    "print(\"  - Still sensitive to translations, rotations\")\n",
    "print(\"  - Better with feature extraction (e.g., HOG, SIFT, CNN features)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0f766e-e313-4c28-a2af-b8a7985e3db7",
   "metadata": {},
   "source": [
    "### Problem 3: Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ca95dc-c37e-4f56-ab0a-9913bde3079f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Problem 3: Perceptron Analysis (Part 1)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n1. Evaluating h(x) = sign(w·x) on D_TR and D_TE\")\n",
    "print(\"-\" * 60)\n",
    "print(\"Question: Does this help determine if test error > training error?\")\n",
    "print()\n",
    "print(\"Answer: YES, it directly computes both errors.\")\n",
    "print()\n",
    "print(\"Training Error:\")\n",
    "print(\"  error_train = (1/|D_TR|) × Σ 1[h(x) ≠ y] for (x,y) in D_TR\")\n",
    "print()\n",
    "print(\"Test Error:\")\n",
    "print(\"  error_test = (1/|D_TE|) × Σ 1[h(x) ≠ y] for (x,y) in D_TE\")\n",
    "print()\n",
    "print(\"Comparison:\")\n",
    "print(\"  if error_test > error_train: overfitting\")\n",
    "print(\"  if error_test ≈ error_train: good generalization\")\n",
    "print(\"  if error_test < error_train: underfitting (rare) or lucky test set\")\n",
    "print()\n",
    "print(\"This is the STANDARD way to compare errors and detect overfitting.\")\n",
    "\n",
    "print(\"\\n2. Why No Need to Compute Training Error Explicitly?\")\n",
    "print(\"-\" * 60)\n",
    "print(\"Answer: The Perceptron algorithm GUARANTEES zero training error\")\n",
    "print(\"        when it converges (for linearly separable data).\")\n",
    "print()\n",
    "print(\"Reasoning:\")\n",
    "print()\n",
    "print(\"a) CONVERGENCE THEOREM:\")\n",
    "print(\"   - If data is linearly separable, Perceptron converges in finite steps\")\n",
    "print(\"   - Upon convergence, ALL training points are correctly classified\")\n",
    "print(\"   - Therefore: training error = 0\")\n",
    "print()\n",
    "print(\"b) ALGORITHM TERMINATION:\")\n",
    "print(\"   - Perceptron stops when no misclassified points remain\")\n",
    "print(\"   - Stopping condition: ∀(x,y) in D_TR: y × (w·x) > 0\")\n",
    "print(\"   - This means: ∀(x,y) in D_TR: sign(w·x) = y\")\n",
    "print()\n",
    "print(\"c) UPDATE RULE:\")\n",
    "print(\"   - Only updates on mistakes: if y × (w·x) ≤ 0\")\n",
    "print(\"   - When algorithm finishes, no mistakes exist\")\n",
    "print(\"   - Hence: training error = 0 automatically\")\n",
    "print()\n",
    "print(\"Important Notes:\")\n",
    "print(\"  • This ONLY applies to linearly separable data\")\n",
    "print(\"  • For non-separable data, Perceptron may not converge\")\n",
    "print(\"  • In practice, use max iterations to prevent infinite loops\")\n",
    "print(\"  • The final w found depends on initialization and order\")\n",
    "print()\n",
    "print(\"Conclusion: For a FULLY TRAINED Perceptron that converged,\")\n",
    "print(\"           training error is implicitly 0, so only test error matters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8e8682-2b9f-4b15-a38e-2d3ec75591dc",
   "metadata": {},
   "source": [
    "### Problem 3: Two-point 2D Dataset (Part 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd4597a-387e-4d5d-bbe3-f621afd13625",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(\"Problem 3: Perceptron on Two-Point Dataset (Part 2)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Dataset\n",
    "x1 = np.array([10, -2])\n",
    "y1 = 1  # Positive\n",
    "\n",
    "x2 = np.array([12, 2])\n",
    "y2 = -1  # Negative\n",
    "\n",
    "print(\"\\nDataset:\")\n",
    "print(f\"  Positive: x1 = {x1}, y1 = {y1}\")\n",
    "print(f\"  Negative: x2 = {x2}, y2 = {y2}\")\n",
    "print(f\"\\nInitial weight: w0 = [0, 0]\")\n",
    "print(f\"Learning rate: η = 1\")\n",
    "print()\n",
    "\n",
    "# Perceptron algorithm\n",
    "w = np.array([0.0, 0.0])\n",
    "data = [(x1, y1), (x2, y2)]\n",
    "update_count = 0\n",
    "weights_history = [w.copy()]\n",
    "\n",
    "print(\"Perceptron Updates:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "converged = False\n",
    "max_iterations = 100\n",
    "iteration = 0\n",
    "\n",
    "while not converged and iteration < max_iterations:\n",
    "    converged = True\n",
    "    \n",
    "    for x, y in data:\n",
    "        prediction = np.dot(w, x)\n",
    "        \n",
    "        if y * prediction <= 0:\n",
    "            converged = False\n",
    "            update_count += 1\n",
    "            \n",
    "            print(f\"\\nUpdate #{update_count}:\")\n",
    "            print(f\"  Current w = {w}\")\n",
    "            print(f\"  Point: x = {x}, y = {y}\")\n",
    "            print(f\"  Prediction: w·x = {prediction:.2f}\")\n",
    "            print(f\"  y × (w·x) = {y * prediction:.2f} ≤ 0  →  MISCLASSIFIED\")\n",
    "            \n",
    "            # Update rule: w = w + η × y × x\n",
    "            w = w + y * x\n",
    "            weights_history.append(w.copy())\n",
    "            \n",
    "            print(f\"  Update: w_new = w_old + {y} × {x}\")\n",
    "            print(f\"  New w = {w}\")\n",
    "    \n",
    "    iteration += 1\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CONVERGENCE ACHIEVED!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nTotal updates: {update_count}\")\n",
    "print(f\"Final weight vector: w = {w}\")\n",
    "\n",
    "print(\"\\nVerification:\")\n",
    "print(\"-\" * 60)\n",
    "for x, y in data:\n",
    "    prediction = np.dot(w, x)\n",
    "    sign_pred = 1 if prediction > 0 else -1\n",
    "    correct = \"✓\" if sign_pred == y else \"✗\"\n",
    "    print(f\"  x = {x}: w·x = {prediction:.2f}, sign(w·x) = {sign_pred:2d}, y = {y:2d}  {correct}\")\n",
    "\n",
    "print(\"\\nSequence of Weight Vectors:\")\n",
    "print(\"-\" * 60)\n",
    "for i, w_i in enumerate(weights_history):\n",
    "    print(f\"  w{i} = {w_i}\")\n",
    "\n",
    "print(\"\\nDecision Boundary:\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"  Equation: {w[0]:.0f}×x₁ + {w[1]:.0f}×x₂ = 0\")\n",
    "if w[1] != 0:\n",
    "    slope = -w[0] / w[1]\n",
    "    print(f\"  Slope: {slope:.2f}\")\n",
    "    print(f\"  Line: x₂ = {slope:.2f}×x₁\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba29c20-59b0-456f-994e-05897175596e",
   "metadata": {},
   "source": [
    "### Problem 4: Reconstructing the Weight Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fb261e-d6ba-4ecd-a4f4-e9b6f5104079",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(\"Problem 4: Reconstructing Weight Vector from Update Log\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Update log\n",
    "updates = [\n",
    "    (np.array([0, 0, 0, 0, 4]), 1, 2),\n",
    "    (np.array([0, 0, 6, 5, 0]), 1, 1),\n",
    "    (np.array([3, 0, 0, 0, 0]), -1, 1),\n",
    "    (np.array([0, 9, 3, 6, 0]), -1, 1),\n",
    "    (np.array([0, 1, 0, 2, 5]), -1, 1)\n",
    "]\n",
    "\n",
    "print(\"\\nGiven Information:\")\n",
    "print(f\"  Initial weight: w0 = [0, 0, 0, 0, 0]\")\n",
    "print(f\"  Learning rate: η = 1\")\n",
    "print()\n",
    "print(\"Update Log:\")\n",
    "print(\"  x               | y  | count\")\n",
    "print(\"  \" + \"-\"*40)\n",
    "for x, y, count in updates:\n",
    "    print(f\"  {str(x):15s} | {y:2d} | {count}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Perceptron Update Rule: w_new = w_old + η × y × x\")\n",
    "print(\"With η = 1: w_new = w_old + y × x\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Initialize weight\n",
    "w = np.array([0, 0, 0, 0, 0])\n",
    "print(f\"\\nw0 = {w}\")\n",
    "\n",
    "# Apply updates\n",
    "update_num = 0\n",
    "for x, y, count in updates:\n",
    "    for i in range(count):\n",
    "        update_num += 1\n",
    "        print(f\"\\nUpdate #{update_num}:\")\n",
    "        print(f\"  w_old = {w}\")\n",
    "        print(f\"  x = {x}, y = {y}\")\n",
    "        print(f\"  Δw = {y} × {x} = {y * x}\")\n",
    "        \n",
    "        w = w + y * x\n",
    "        print(f\"  w_new = {w}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL RESULT\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nFinal weight vector after all {update_num} updates:\")\n",
    "print(f\"w_final = {w}\")\n",
    "print()\n",
    "print(\"Breakdown by dimension:\")\n",
    "for i in range(5):\n",
    "    print(f\"  w[{i}] = {w[i]}\")\n",
    "\n",
    "# Verification\n",
    "print(\"\\nManual Calculation Verification:\")\n",
    "print(\"-\" * 60)\n",
    "w_manual = np.array([0, 0, 0, 0, 0])\n",
    "print(\"Starting from w = [0, 0, 0, 0, 0]:\")\n",
    "print(f\"  + 2 × (+1) × [0,0,0,0,4] = [0, 0, 0, 0, 8]\")\n",
    "print(f\"  + 1 × (+1) × [0,0,6,5,0] = [0, 0, 6, 5, 8]\")\n",
    "print(f\"  + 1 × (-1) × [3,0,0,0,0] = [-3, 0, 6, 5, 8]\")\n",
    "print(f\"  + 1 × (-1) × [0,9,3,6,0] = [-3, -9, 3, -1, 8]\")\n",
    "print(f\"  + 1 × (-1) × [0,1,0,2,5] = [-3, -10, 3, -3, 3]\")\n",
    "print(f\"\\n  Final: w = [-3, -10, 3, -3, 3] ✓\")
  },
  {
   "cell_type": "markdown",
   "id": "92f23b69-9f59-46c6-8103-5783fadeb7c0",
   "metadata": {},
   "source": [
    "### Problem 5: Visualizing Perceptron Convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9879a3a9-de75-40a0-a901-bd2009d2b5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML\n",
    "\n",
    "print(\"Problem 5: Visualizing Perceptron Convergence\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create a linearly separable 2D dataset\n",
    "np.random.seed(42)\n",
    "\n",
    "# Positive class (upper right)\n",
    "positive = np.array([\n",
    "    [3, 4], [4, 5], [5, 4], [4, 6], [6, 5],\n",
    "    [5, 6], [6, 6], [7, 5]\n",
    "])\n",
    "\n",
    "# Negative class (lower left)\n",
    "negative = np.array([\n",
    "    [1, 1], [2, 1], [1, 2], [2, 2], [3, 1],\n",
    "    [1, 3], [2, 3], [3, 2]\n",
    "])\n",
    "\n",
    "# Combine data\n",
    "X = np.vstack([positive, negative])\n",
    "y = np.array([1]*len(positive) + [-1]*len(negative))\n",
    "\n",
    "print(f\"\\nDataset created:\")\n",
    "print(f\"  Positive examples: {len(positive)}\")\n",
    "print(f\"  Negative examples: {len(negative)}\")\n",
    "print(f\"  Total examples: {len(X)}\")\n",
    "\n",
    "# Perceptron algorithm with history tracking\n",
    "def perceptron_with_history(X, y, learning_rate=1.0, max_iter=100):\n",
    "    w = np.array([0.0, 0.0])\n",
    "    history = [{'w': w.copy(), 'misclassified': [], 'iteration': 0}]\n",
    "    \n",
    "    for iteration in range(max_iter):\n",
    "        misclassified = []\n",
    "        converged = True\n",
    "        \n",
    "        for i, (xi, yi) in enumerate(zip(X, y)):\n",
    "            prediction = np.dot(w, xi)\n",
    "            \n",
    "            if yi * prediction <= 0:\n",
    "                misclassified.append(i)\n",
    "                converged = False\n",
    "                w = w + learning_rate * yi * xi\n",
    "                history.append({\n",
    "                    'w': w.copy(),\n",
    "                    'misclassified': [i],\n",
    "                    'iteration': iteration + 1,\n",
    "                    'updated_point': i\n",
    "                })\n",
    "        \n",
    "        if converged:\n",
    "            print(f\"\\nConverged after {iteration + 1} iterations!\")\n",
    "            break\n",
    "    \n",
    "    return w, history\n",
    "\n",
    "# Run perceptron\n",
    "w_final, history = perceptron_with_history(X, y)\n",
    "\n",
    "print(f\"Total updates: {len(history) - 1}\")\n",
    "print(f\"Final weight: w = {w_final}\")\n",
    "\n",
    "# Visualization\n",
    "def plot_perceptron_state(ax, X, y, positive, negative, w, step, title):\n",
    "    ax.clear()\n",
    "    \n",
    "    # Plot data points\n",
    "    ax.scatter(positive[:, 0], positive[:, 1], c='blue', s=150, \n",
    "               marker='o', edgecolors='black', linewidths=2, \n",
    "               label='Positive (+1)', zorder=3)\n",
    "    ax.scatter(negative[:, 0], negative[:, 1], c='red', s=150, \n",
    "               marker='s', edgecolors='black', linewidths=2, \n",
    "               label='Negative (-1)', zorder=3)\n",
    "    \n",
    "    # Plot decision boundary\n",
    "    if not np.allclose(w, 0):\n",
    "        x_min, x_max = 0, 8\n",
    "        \n",
    "        if abs(w[1]) > 1e-6:\n",
    "            # w[0]*x + w[1]*y = 0  =>  y = -(w[0]/w[1])*x\n",
    "            x_line = np.array([x_min, x_max])\n",
    "            y_line = -(w[0] / w[1]) * x_line\n",
    "            ax.plot(x_line, y_line, 'g-', linewidth=2.5, \n",
    "                    label=f'w·x=0', zorder=2)\n",
    "            ax.plot(x_line, y_line, 'g--', linewidth=1, alpha=0.5, zorder=1)\n",
    "        else:\n",
    "            # Vertical line at x = 0\n",
    "            ax.axvline(x=0, color='g', linewidth=2.5, label='w·x=0', zorder=2)\n",
    "    \n",
    "    ax.set_xlim(0, 8)\n",
    "    ax.set_ylim(0, 7)\n",
    "    ax.set_xlabel('Feature 1 (x₁)', fontsize=11)\n",
    "    ax.set_ylabel('Feature 2 (x₂)', fontsize=11)\n",
    "    ax.set_title(title, fontsize=13, fontweight='bold')\n",
    "    ax.legend(loc='upper left', fontsize=9)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "# Create figure with multiple subplots showing key steps\n",
    "key_steps = [0, len(history)//4, len(history)//2, 3*len(history)//4, -1]\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, step in enumerate(key_steps):\n",
    "    state = history[step]\n",
    "    title = f\"Step {state['iteration']}: w = [{state['w'][0]:.1f}, {state['w'][1]:.1f}]\"\n",
    "    if step == 0:\n",
    "        title = \"Initial: w = [0.0, 0.0]\"\n",
    "    elif step == -1:\n",
    "        title = f\"Converged: w = [{state['w'][0]:.1f}, {state['w'][1]:.1f}]\"\n",
    "    \n",
    "    plot_perceptron_state(axes[idx], X, y, positive, negative, \n",
    "                          state['w'], step, title)\n",
    "\n",
    "# Remove extra subplot\n",
    "fig.delaxes(axes[5])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nVisualization complete!\")\n",
    "print(\"\\nKey observations:\")\n",
    "print(\"  • Initial boundary (w=0) doesn't separate classes\")\n",
    "print(\"  • Each update rotates the decision boundary\")\n",
    "print(\"  • Boundary moves to correctly classify misclassified points\")\n",
    "print(\"  • Final boundary perfectly separates positive and negative classes\")\n",
    "print(\"  • Training error = 0 at convergence\")\n",
    "\n",
    "# Detailed history\n",
    "print(\"\\nDetailed Update History:\")\n",
    "print(\"-\" * 60)\n",
    "for i, state in enumerate(history[:10]):  # Show first 10 updates\n",
    "    w = state['w']\n",
    "    print(f\"Step {i}: w = [{w[0]:6.2f}, {w[1]:6.2f}]\")\n",
    "if len(history) > 10:\n",
    "    print(f\"... ({len(history) - 10} more updates)\")\n",
    "    final_state = history[-1]\n",
    "    w = final_state['w']\n",
    "    print(f\"Step {len(history)-1}: w = [{w[0]:6.2f}, {w[1]:6.2f}]  [CONVERGED]\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}